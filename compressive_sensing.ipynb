{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\color{teal}{\\text{Compressive Sensing}}$ \n",
    "### $\\color{gray}{\\text{1. Introduction}}$\n",
    "\n",
    "#### $\\color{gray}{\\text{1.1 Problem Setting}}$\n",
    "In signal processing we are often interested in reconstructing a signal $x \\in \\mathbb{C}^N$ given some observation data $y \\in \\mathbb{C}^N$. By assuming linear measurements we can state the linear system of equations $Ax = y$. Here, the matrix $A \\in \\mathbb{C}^{m \\times N}$ denotes the measurement process and is often referred to as the measurement matrix. Solving this linear system depends on the dimensions of $A$. If the number of measurements $m$ is at least as large as the signal length $N$ we can solve the well-defined system of equations. However, if we consider the case that $m<N$, i.e. $A$ is a wide matrix, we get an underdetermined linear system. Due to the ill-posedness of the underdetermined system $Ax=y$, the original problem becomes an infinite dimensional problem with infinitely many solutions. Compressive Sensing (CS) introduces certain assumptions on the signal $x$ and provides the theory to recover $x$ when the system $Ax=y$ is underdetermined. The underlying assumption is sparsity, restricting solutions to vectors that have few nonzero entries.\n",
    "\n",
    "####  $\\color{gray}{\\text{1.1 Sparsity}}$\n",
    "A vector is $s$-sparse if it has at most $s$ nonzero elements. Using the notion of the $\\mathcal{l}_0$-norm denoted by $||x||_0$, we count the number of nonzero elements of the vector $x \\in \\mathbb{C}^N$. As it turns out, many real-world signals become sparse when properly transformed - this is the case in image compression. In the following code cell we compress an image by applying the Discrete Wavelet Transform (DWT) and reconstruct the image with a small amount of wavelet coefficients. For the reconstruction we used only the strongest wavelet coefficients as all other coefficients were set to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = r\".\\images\\kiz_kulesi.jpg\"\n",
    "\n",
    "image = Image.open(path).convert(\"L\")\n",
    "arr = np.asarray(image)\n",
    "plt.imshow(arr, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pywavelets.readthedocs.io/en/latest/\n",
    "import pywt\n",
    "\n",
    "titles = ['Approximation', ' Horizontal detail',\n",
    "          'Vertical detail', 'Diagonal detail']\n",
    "wavelet = pywt.Wavelet('db1')\n",
    "\n",
    "# Multilevel decomposition of the input matrix\n",
    "coeffs2 = pywt.dwt2(arr, wavelet, )\n",
    "cA2, (cH2, cV2, cD2), (cH1, cV1, cD1) = coeffs\n",
    "\n",
    "# Concatenate the level-2 submatrices into a big one and plot\n",
    "x_house_star_wav = np.bmat([[cA2, cH2], [cV2, cD2]])\n",
    "LL, (LH, HL, HH) = coeffs2\n",
    "fig = plt.figure(figsize=(12, 3))\n",
    "for i, a in enumerate([LL, LH, HL, HH]):\n",
    "    ax = fig.add_subplot(1, 4, i + 1)\n",
    "    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n",
    "    ax.set_title(titles[i], fontsize=10)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we exploted the compressibility of natural images. In practice we usually deal with vectors that are not exactly $s$-sparse. Due to their compressibility we can still find a suitable approximation which is quantified by the error of the best $s$-term approximation \n",
    "\n",
    "$$\\sigma_s(x)_p = \\underset{||z||_0 \\leq s}{\\mathrm{inf}} ||x-z||_p$$\n",
    "\n",
    "where $z$ is a sparse vextor. One might be interested in how many measurements we need to recover all $s$-sparse vectors x $-$ spoiler: $m = 2s$ measurements.\n",
    "\n",
    "####  $\\color{gray}{\\text{1.2 The } \\mathcal{l}_0 \\text{-Minimization Problem}}$\n",
    "With this insight, the first intuitive approach is to search for the sparsest vector satisfying $Az=y$. This can be formulated as the $\\mathcal{l}_0$-minimization problem \n",
    "\n",
    "$$ \\text{minimize } ||z||_0 \\;\\;\\; \\text{subject to } Az=y$$\n",
    "\n",
    "which we denote as $\\color{green}{(P_0)}$. In practical applications we have to deal with noisy measurements which may affect the stability of our algorithms. By tolerating an error value $\\eta$ we can introduced the less strict optimization problem\n",
    "\n",
    "$$ \\text{minimize } ||z||_0 \\;\\;\\; \\text{subject to } ||Az-y||_2 \\leq \\eta$$\n",
    "\n",
    "as $\\color{green}{(P_{0,\\eta})}$. Unfortunately, both problems ar NP-hard which means that there are no efficient algorithms to solve them and thus unhelpful in practice. The corresponding proof is based on the exact cover by $3$-sets problem, which is NP-complete. At first glance it is rather surprising that efficient recovery algorithms do exist. These can be categorized into optimization methods, greedy methods and thresholding-based methods.\n",
    "\n",
    "#### $\\color{gray}{\\text{1.3 Basis Pursuit}}$\n",
    "Instead of solving the $\\color{green}{(P_0)}$ we can consider a manageable approximation given as \n",
    "\n",
    "$$ \\text{minimize } ||z||_1 \\;\\;\\; \\text{subject to } Az=y$$\n",
    "\n",
    "which we denote with $\\color{green}{(P_1)}$. One important property of the $\\mathcal{l}_1$-norm is that it favors sparse solutions. It turns out that the basis pursuit (also called $\\mathcal{l}_1$-minimization) is a convex optimization problem, for which efficient solving algorithms exist. \n",
    "\n",
    "#### $\\color{gray}{\\text{1.4 }}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\color{gray}{\\text{Algorithms}}$\n",
    "Alternative reconstruction\n",
    "methods include greedy-type methods such as orthogonal matching pursuit, as well\n",
    "6 1 An Invitation to Compressive Sensing\n",
    "as thresholding-based methods including iterative hard thresholding (Foucart p.6)\n",
    "\n",
    "####  $\\color{gray}{\\text{References}}$\n",
    "[1] Mathematical Introducation to Compressive Sensing, Simon Foucart, Holger Rauhut"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
