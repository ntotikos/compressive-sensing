{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\color{teal}{\\text{Compressive Sensing}}$ \n",
    "### $\\color{gray}{\\text{1. Introduction}}$\n",
    "\n",
    "#### $\\color{gray}{\\text{1.1 Problem Setting}}$\n",
    "In signal processing we are often interested in reconstructing a signal $x \\in \\mathbb{C}^N$ given some observation data $y \\in \\mathbb{C}^N$. By assuming linear measurements we can state the linear system of equations $Ax = y$. Here, the matrix $A \\in \\mathbb{C}^{m \\times N}$ denotes the measurement process and is often referred to as the measurement matrix. Solving this linear system depends on the dimensions of $A$. If the number of measurements $m$ is at least as large as the signal length $N$ we can solve the well-defined system of equations. However, if we consider the case that $m<N$, i.e. $A$ is a wide matrix, we get an underdetermined linear system. Due to the ill-posedness of the underdetermined system $Ax=y$, the original problem becomes an infinite dimensional problem with infinitely many solutions. Compressive Sensing (CS) introduces certain assumptions on the signal $x$ and provides the theory to recover $x$ when the system $Ax=y$ is underdetermined. The underlying assumption is sparsity, restricting solutions to vectors that have few nonzero entries.\n",
    "\n",
    "####  $\\color{gray}{\\text{1.1 Sparsity}}$\n",
    "A vector is $s$-sparse if it has at most $s$ nonzero elements. Using the notion of the $\\mathcal{l}_0$-norm denoted by $||x||_0$, we count the number of nonzero elements of the vector $x \\in \\mathbb{C}^N$. As it turns out, many real-world signals become sparse when properly transformed - this is the case in image compression. In the following code cell we compress an image by applying the Discrete Wavelet Transform (DWT) and reconstruct the image with a small amount of wavelet coefficients. For the reconstruction we used only the strongest wavelet coefficients as all other coefficients were set to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = r\".\\images\\kiz_kulesi.jpg\"\n",
    "\n",
    "image = Image.open(path).convert(\"L\")\n",
    "arr = np.asarray(image)\n",
    "plt.imshow(arr, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pywavelets.readthedocs.io/en/latest/\n",
    "import pywt\n",
    "\n",
    "titles = ['Approximation', ' Horizontal detail',\n",
    "          'Vertical detail', 'Diagonal detail']\n",
    "wavelet = pywt.Wavelet('db1')\n",
    "\n",
    "# Multilevel decomposition of the input matrix\n",
    "coeffs2 = pywt.dwt2(arr, wavelet, )\n",
    "cA2, (cH2, cV2, cD2), (cH1, cV1, cD1) = coeffs\n",
    "\n",
    "# Concatenate the level-2 submatrices into a big one and plot\n",
    "x_house_star_wav = np.bmat([[cA2, cH2], [cV2, cD2]])\n",
    "LL, (LH, HL, HH) = coeffs2\n",
    "fig = plt.figure(figsize=(12, 3))\n",
    "for i, a in enumerate([LL, LH, HL, HH]):\n",
    "    ax = fig.add_subplot(1, 4, i + 1)\n",
    "    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n",
    "    ax.set_title(titles[i], fontsize=10)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we exploted the compressibility of natural images. In practice we usually deal with vectors that are not exactly $s$-sparse. Due to their compressibility we can still find a suitable approximation which is quantified by the error of the best $s$-term approximation \n",
    "\n",
    "$$\\sigma_s(x)_p = \\underset{||z||_0 \\leq s}{\\mathrm{inf}} ||x-z||_p$$\n",
    "\n",
    "where $z$ is a sparse vextor. One might be interested in how many measurements we need to recover all $s$-sparse vectors x $-$ spoiler: $m = 2s$ measurements.\n",
    "\n",
    "####  $\\color{gray}{\\text{1.2 The } \\mathcal{l}_0 \\text{-Minimization Problem}}$\n",
    "With this insight, the first intuitive approach is to search for the sparsest vector satisfying $Az=y$. This can be formulated as the $\\mathcal{l}_0$-minimization problem \n",
    "\n",
    "$$ \\text{minimize } ||z||_0 \\;\\;\\; \\text{subject to } Az=y$$\n",
    "\n",
    "which we denote as $\\color{green}{(P_0)}$. In practical applications we have to deal with noisy measurements which may affect the stability of our algorithms. By tolerating an error value $\\eta$ we can introduced the less strict optimization problem\n",
    "\n",
    "$$ \\text{minimize } ||z||_0 \\;\\;\\; \\text{subject to } ||Az-y||_2 \\leq \\eta$$\n",
    "\n",
    "as $\\color{green}{(P_{0,\\eta})}$. Unfortunately, both problems ar NP-hard which means that there are no efficient algorithms to solve them and thus unhelpful in practice. The corresponding proof is based on the exact cover by $3$-sets problem, which is NP-complete. At first glance it is rather surprising that efficient recovery algorithms do exist. These can be categorized into optimization methods, greedy methods and thresholding-based methods.\n",
    "\n",
    "#### $\\color{gray}{\\text{1.3 Basis Pursuit}}$\n",
    "Instead of solving the $\\color{green}{(P_0)}$ we can consider a manageable approximation given as \n",
    "\n",
    "$$ \\text{minimize } ||z||_1 \\;\\;\\; \\text{subject to } Az=y$$\n",
    "\n",
    "which we denote with $\\color{green}{(P_1)}$. One important property of the $\\mathcal{l}_1$-norm is that it favors sparse solutions. It turns out that the basis pursuit (also called $\\mathcal{l}_1$-minimization) is a convex optimization problem, for which efficient solving algorithms exist. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\color{gray}{\\text{Iterative Hard Thresholding}}$\n",
    "The so-called restricted isometry property (RIP) of matrices gives a hint on whether the sparse recovery algorithm will succeed or not. It is NP-hard to show that a given matrix fullfilles the RIP, but fortunately a theorem assures that subgaussian random matrices with i.i.d. entries satisfy the RIP with high probability. Therefore, we are going to use the numpy function `random.randn()` to generate the corresponding measurement matrix $A$.\n",
    "\n",
    "\n",
    "The iterative hard thresholding (IHT) algorithm is based on non-convex optimization. Hence, it converges to local minima and depends on the initialization of `x_0` (commomly with the all-zero vector). In following we will implement the general setup of CS, i.e. generate the measurement matrix $A$ and the measurements $y$ based on the true vector `x_true` that is randomly generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "m = 500\n",
    "N = 1000\n",
    "s = 10\n",
    "\n",
    "#ground truth\n",
    "x_true = np.zeros(N)\n",
    "indices = random.sample(range(1000),s) #randomly sample s out of 1000 indices\n",
    "x_true[indices] = np.random.rand(s)\n",
    "\n",
    "#random measurement matrix\n",
    "A = np.random.randn(m,N)\n",
    "A = A/math.sqrt(m)\n",
    "#simulated measurements\n",
    "y = A @ x_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each iteration of the IHT algorithm we calculate \n",
    "\n",
    "$$x^{(k+1)} = \\mathcal{H}_s\\{x^{(k)} + A^{*}(y-Ax^{(k)})\\}$$\n",
    "\n",
    "where $\\mathcal{H}_s$ denotes the hard thresholding operator of order $s$ and $A^{*}$ the hermitian of $A$. By applying $\\mathcal{H}_s$ we take only the $s$ entries with the largest absolute values and set the other ones to zero, thus ensuring the sparsity $s$ of $x^{(k+1)}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H_s(x_tmp,s):\n",
    "    \"\"\"Hard-thresholding operator: keep the s largest entries of z and set the other ones to zero.\"\"\"\n",
    "    N = x_tmp.shape[0]\n",
    "    x_new = np.zeros(N)\n",
    "    indices = x_tmp.argsort()[-s:]\n",
    "    x_new[indices] = x_tmp[indices]\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iht(A, y, s, iters):\n",
    "    \"\"\"Implementation of the IHT.\"\"\"\n",
    "    [m,N] = A.shape\n",
    "    x_0 = np.zeros(N)\n",
    "    \n",
    "    #hermitian of A\n",
    "    A_H = A.conjugate().transpose()\n",
    "    \n",
    "    x_iters = []\n",
    "    err_iters = np.zeros(iters)\n",
    "    x_hat = np.zeros(N)\n",
    "    \n",
    "    for i in range(iters):\n",
    "        if i==0:\n",
    "            x_k = x_0\n",
    "        else:\n",
    "            x_k = H_s(x_k + A_H@(y - A@x_k),s)\n",
    "            #@TODO: iteratively plot x_k and x_true\n",
    "            #print(f\"x_hat_{i}: {[val for val in x_k if val!=0]}\\n\")\n",
    "            x_hat = x_k\n",
    "        x_iters.append(x_k)\n",
    "        \n",
    "        err = np.linalg.norm(A@x_k-y,2) #l2-error\n",
    "        err_iters[i] = err\n",
    "        print(f\"err iteration {i}: {err}\")\n",
    "    return x_hat, x_iters, err_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_iters = 50\n",
    "x_hat, x_iters, err_iters = iht(A,y,s,n_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#@TODO: iteratively plot x_k and x_true\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "t = np.linspace(0,999,1000)\n",
    "\n",
    "plt.plot(t, x_hat, 'r') # plotting t, a separately \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/21360361/how-to-dynamically-update-a-plot-in-a-loop-in-ipython-notebook-within-one-cell\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "for i in range(10):\n",
    "    pl.plot(pl.randn(100))\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(pl.gcf())\n",
    "    time.sleep(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\color{gray}{\\text{Algorithms}}$\n",
    "Alternative reconstruction\n",
    "methods include greedy-type methods such as orthogonal matching pursuit, as well\n",
    "6 1 An Invitation to Compressive Sensing\n",
    "as thresholding-based methods including iterative hard thresholding (Foucart p.6)\n",
    "\n",
    "####  $\\color{gray}{\\text{References}}$\n",
    "[1] Mathematical Introducation to Compressive Sensing, Simon Foucart, Holger Rauhut"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
