{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\color{teal}{\\text{Compressive Sensing}}$ \n",
    "### $\\color{gray}{\\text{1. Introduction}}$\n",
    "\n",
    "#### $\\color{gray}{\\text{1.1 Problem Setting}}$\n",
    "In signal processing we are often interested in reconstructing a signal $x \\in \\mathbb{C}^N$ given some observation data $y \\in \\mathbb{C}^N$. By assuming linear measurements we can state the linear system of equations $Ax = y$. Here, the matrix $A \\in \\mathbb{C}^{m \\times N}$ denotes the measurement process and is often referred to as the measurement matrix. Solving this linear system depends on the dimensions of $A$. If the number of measurements $m$ is at least as large as the signal length $N$ we can solve the well-defined system of equations. However, if we consider the case that $m<N$, i.e. $A$ is a wide matrix, we get an underdetermined linear system. Due to the ill-posedness of the underdetermined system $Ax=y$, the original problem becomes an infinite dimensional problem with infinitely many solutions. Compressive Sensing (CS) introduces certain assumptions on the signal $x$ and provides the theory to recover $x$ when the system $Ax=y$ is underdetermined. The underlying assumption is sparsity, restricting solutions to vectors that have few nonzero entries.\n",
    "\n",
    "####  $\\color{gray}{\\text{1.1 Sparsity}}$\n",
    "A vector is $s$-sparse if it has at most $s$ nonzero elements. Using the notion of the $\\mathcal{l}_0$-norm denoted by $||x||_0$, we count the number of nonzero elements of the vector $x \\in \\mathbb{C}^N$. As it turns out, many real-world signals become sparse when properly transformed - this is the case in image compression. In the following code cell we compress an image by applying the Discrete Wavelet Transform (DWT) and reconstruct the image with a small amount of wavelet coefficients. For the reconstruction we used only the strongest wavelet coefficients as all other coefficients were set to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = r\".\\images\\kiz_kulesi.jpg\"\n",
    "\n",
    "image = Image.open(path).convert(\"L\")\n",
    "data = np.asarray(image)\n",
    "plt.imshow(data, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform image to numpyarray and sparsify it\n",
    "path = r\".\\images\\pokeball.png\"\n",
    "\n",
    "image = Image.open(path).convert(\"L\")\n",
    "np_data = np.asarray(image)\n",
    "plt.imshow(np_data, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()\n",
    "\n",
    "[rows,clmns] = np_data.shape\n",
    "index_row = np.arange(rows)\n",
    "index_clm = np.arange(clmns)\n",
    "\n",
    "sparse_A = np.zeros((rows,50))\n",
    "sparse_A[:,index_clm] = np_data[:,index_clm]\n",
    "\n",
    "plt.imshow(sparse_A, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pywavelets.readthedocs.io/en/latest/\n",
    "import pywt\n",
    "\n",
    "titles = ['Approximation', ' Horizontal detail',\n",
    "          'Vertical detail', 'Diagonal detail']\n",
    "wavelet = pywt.Wavelet('db1')\n",
    "\n",
    "# Multilevel decomposition of the input matrix\n",
    "coeffs2 = pywt.dwt2(arr, wavelet, )\n",
    "cA2, (cH2, cV2, cD2), (cH1, cV1, cD1) = coeffs\n",
    "\n",
    "# Concatenate the level-2 submatrices into a big one and plot\n",
    "x_house_star_wav = np.bmat([[cA2, cH2], [cV2, cD2]])\n",
    "LL, (LH, HL, HH) = coeffs2\n",
    "fig = plt.figure(figsize=(12, 3))\n",
    "for i, a in enumerate([LL, LH, HL, HH]):\n",
    "    ax = fig.add_subplot(1, 4, i + 1)\n",
    "    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n",
    "    ax.set_title(titles[i], fontsize=10)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we exploted the compressibility of natural images. In practice we usually deal with vectors that are not exactly $s$-sparse. Due to their compressibility we can still find a suitable approximation which is quantified by the error of the best $s$-term approximation \n",
    "\n",
    "$$\\sigma_s(x)_p = \\underset{||z||_0 \\leq s}{\\mathrm{inf}} ||x-z||_p$$\n",
    "\n",
    "where $z$ is a sparse vextor. One might be interested in how many measurements we need to recover all $s$-sparse vectors x $-$ spoiler: $m = 2s$ measurements.\n",
    "\n",
    "####  $\\color{gray}{\\text{1.2 The } \\mathcal{l}_0 \\text{-Minimization Problem}}$\n",
    "With this insight, the first intuitive approach is to search for the sparsest vector satisfying $Az=y$. This can be formulated as the $\\mathcal{l}_0$-minimization problem \n",
    "\n",
    "$$ \\text{minimize } ||z||_0 \\;\\;\\; \\text{subject to } Az=y$$\n",
    "\n",
    "which we denote as $\\color{green}{(P_0)}$. In practical applications we have to deal with noisy measurements which may affect the stability of our algorithms. By tolerating an error value $\\eta$ we can introduced the less strict optimization problem\n",
    "\n",
    "$$ \\text{minimize } ||z||_0 \\;\\;\\; \\text{subject to } ||Az-y||_2 \\leq \\eta$$\n",
    "\n",
    "as $\\color{green}{(P_{0,\\eta})}$. Unfortunately, both problems ar NP-hard which means that there are no efficient algorithms to solve them and thus unhelpful in practice. The corresponding proof is based on the exact cover by $3$-sets problem, which is NP-complete. At first glance it is rather surprising that efficient recovery algorithms do exist. These can be categorized into optimization methods, greedy methods and thresholding-based methods.\n",
    "\n",
    "#### $\\color{gray}{\\text{1.3 Basis Pursuit}}$\n",
    "Instead of solving the $\\color{green}{(P_0)}$ we can consider a manageable approximation given as \n",
    "\n",
    "$$ \\text{minimize } ||z||_1 \\;\\;\\; \\text{subject to } Az=y$$\n",
    "\n",
    "which we denote with $\\color{green}{(P_1)}$. One important property of the $\\mathcal{l}_1$-norm is that it favors sparse solutions. It turns out that the basis pursuit (also called $\\mathcal{l}_1$-minimization) is a convex optimization problem, for which efficient solving algorithms exist. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\color{gray}{\\text{Iterative Hard Thresholding}}$\n",
    "The so-called restricted isometry property (RIP) of matrices gives a hint on whether the sparse recovery algorithm will succeed or not. It is NP-hard to show that a given matrix fullfilles the RIP, but fortunately a theorem assures that subgaussian random matrices with i.i.d. entries satisfy the RIP with high probability. Therefore, we are going to use the numpy function `random.randn()` to generate the corresponding measurement matrix $A$.\n",
    "\n",
    "\n",
    "The iterative hard thresholding (IHT) algorithm is based on non-convex optimization. Hence, it converges to local minima and depends on the initialization of `x_0` (commomly with the all-zero vector). In following we will implement the general setup of CS, i.e. generate the measurement matrix $A$ and the measurements $y$ based on the true vector `x_true` that is randomly generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "#iht:\n",
    "m = 300\n",
    "N = 1000\n",
    "s = 10\n",
    "\n",
    "#omp\n",
    "#m = 200\n",
    "#N = 175\n",
    "#s = 30\n",
    "\n",
    "#ground truth\n",
    "x_true = np.zeros(N)\n",
    "indices = random.sample(range(N),s) #randomly sample s out of 1000 indices\n",
    "x_true[indices] = np.random.rand(s)\n",
    "\n",
    "#random measurement matrix\n",
    "A = np.random.randn(m,N)\n",
    "A = A/math.sqrt(m)\n",
    "#simulated measurements\n",
    "y = A @ x_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each iteration of the IHT algorithm we calculate \n",
    "\n",
    "$$x^{(k+1)} = \\mathcal{H}_s\\{x^{(k)} + A^{*}(y-Ax^{(k)})\\}$$\n",
    "\n",
    "where $\\mathcal{H}_s$ denotes the hard thresholding operator of order $s$ and $A^{*}$ the hermitian of $A$. By applying $\\mathcal{H}_s$ we take only the $s$ entries with the largest absolute values and set the other ones to zero, thus ensuring the sparsity $s$ of $x^{(k+1)}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H_s(x_tmp,s):\n",
    "    \"\"\"Hard-thresholding operator: keep the s largest entries of z and set the other ones to zero.\"\"\"\n",
    "    N = x_tmp.shape[0]\n",
    "    x_new = np.zeros(N)\n",
    "    indices = x_tmp.argsort()[-s:]\n",
    "    x_new[indices] = x_tmp[indices]\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iht(A, y, s, iters):\n",
    "    \"\"\"Implementation of the IHT.\"\"\"\n",
    "    [m,N] = A.shape\n",
    "    x_0 = np.zeros(N)\n",
    "    \n",
    "    #hermitian of A\n",
    "    A_H = A.conjugate().transpose()\n",
    "    \n",
    "    x_iters = []\n",
    "    err_iters = np.zeros(iters)\n",
    "    x_hat = np.zeros(N)\n",
    "    \n",
    "    from tqdm import tqdm\n",
    "    for i in tqdm(range(iters)):\n",
    "        if i==0:\n",
    "            x_k = x_0\n",
    "        else:\n",
    "            x_k = H_s(x_k + A_H@(y - A@x_k),s)\n",
    "            #@TODO: iteratively plot x_k and x_true\n",
    "            #print(f\"x_hat_{i}: {[val for val in x_k if val!=0]}\\n\")\n",
    "            x_hat = x_k\n",
    "        x_iters.append(x_k)\n",
    "        \n",
    "        err = np.linalg.norm(A@x_k-y,2) #l2-error\n",
    "        err_iters[i] = err\n",
    "        print(f\"err iteration {i}: {err}\")\n",
    "    return x_hat, x_iters, err_iters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the iterative hard thresholding algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_iters = 100\n",
    "x_hat, x_iters, err_iters = iht(B,z,10,n_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = np.linalg.norm(x_hat-x_true,2) #l2-error\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\color{gray}{\\text{Orthogonal Matching Pursuit}}$\n",
    "This algorithm is a representant of so-called greedy methods. Since the original $\\color{green}{(P_{0,\\eta})}$ is NP-hard, we use the OMP as an approximation method. It iteratively constructs the support set of the reconstructed sparse vector by extending the current support set with an additional index at each iteration. One drawback of this algorithm is that if a wrong index has been selected, it will remain in all subsequent index sets. The initial support set shall be the empty set and $x^{(0)}$ shall be equal to the all-zero vector. Further, the residual $r^{(0)}$ is chosen to be $y$.\n",
    "\n",
    "Each iteration of the OMP algorithms covers mainly two steps denoted by `OMP_1` and `OMP_2` and mathematically expressed as\n",
    "\n",
    "$$ j_{k+1} = \\underset{j \\in [N]}{\\mathrm{argmax}} \\{|a^{*}_j \\cdot r^{(k)}|\\}, \\;\\;\\; S^{(k+1)}=S^{(k)} \\cup \\{j_{k+1}\\}$$\n",
    "and \n",
    "$$ x^{(k+1)} = \\underset{z \\in \\mathbb{C}^{N}}{\\mathrm{argmin}} \\{||y-Az||_2\\} \\;\\;\\; \\text{s.t. supp}(z) \\subset S^{(k+1)} $$\n",
    "\n",
    "We further introduce the residual term $r^{k+1} = y - Ax^{(k+1)}$, which can be understood as an error term. The desired output of the algorithm is an $\\bar{n}$-sparse vector, where $\\bar{n}$ denotes the number of iterations needed for the recovery of $x$. Since `OMP_2` is a least squares problem, we can calculate $x^{(k+1)}$ using the pseudoinverse $A^{\\dagger}_{S^{(k+1)}} \\in \\mathbb{C}^{N \\times m}$ that is restricted to the set $S^{(k+1)}$ via\n",
    "\n",
    "$$ x^{(k+1)}=A^{\\dagger}_{S^{(k+1)}}y = (A^{*}_{S^{(k+1)}}A_{S^{(k+1)}})^{-1}A^{*}_{S^{(k+1)}}y.$$\n",
    "\n",
    "Restricting a matrix on a support set means that that we take all columns indicated in the support set and set the other ones to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "#omp\n",
    "m = 200\n",
    "N = 175\n",
    "s = 30\n",
    "\n",
    "#ground truth\n",
    "x_true = np.zeros(N)\n",
    "indices = random.sample(range(N),s) #randomly sample s out of 1000 indices\n",
    "x_true[indices] = np.random.rand(s)\n",
    "\n",
    "#random measurement matrix\n",
    "A = np.random.randn(m,N)\n",
    "A = A/math.sqrt(m)\n",
    "#simulated measurements\n",
    "y = A @ x_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_index(residual,matrix,n_clmns):\n",
    "    matrix_T = matrix.T\n",
    "    dot_products =[np.dot(matrix_T[i].conjugate(),residual) for i in range(n_clmns)]\n",
    "    abs_vals = np.absolute(dot_products)\n",
    "    return np.argmax(abs_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_on_support(matrix,support_set):\n",
    "    \"\"\"Restrict matrix on support set by copying all elements in the columns indicated by \n",
    "    support_set and setting the other ones to zero.\"\"\"\n",
    "    \n",
    "    [rows,clms] = matrix.shape\n",
    "    matrix_supp = np.zeros((rows,clms), dtype='float32')\n",
    "    matrix_supp[:,support_set] = matrix[:,support_set]\n",
    "    return matrix_supp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def omp(A,y,iters):\n",
    "    \"\"\"Implementation of the Orthogonal Matching Pursuit.\"\"\"   \n",
    "    from numpy.linalg import pinv\n",
    "    \n",
    "    [m,N] = A.shape\n",
    "    #Initializations\n",
    "    x_iters = []\n",
    "    support_set = []\n",
    "    err_iters = np.zeros(iters)\n",
    "    x_0 = np.zeros(N)\n",
    "    r_0 = y\n",
    "    \n",
    "    #Iteration\n",
    "    for i in range(iters):\n",
    "        if i==0:\n",
    "            residual = r_0\n",
    "        print(f\"residual: \\n {residual[50]}\")\n",
    "        index = get_max_index(residual,A,N)\n",
    "        print(f\"index: {index}\")\n",
    "        support_set.append(index)\n",
    "        print(support_set)\n",
    "        \n",
    "        A_r = restrict_on_support(A,support_set)       \n",
    "        x_new = np.linalg.pinv(A_r)@y\n",
    "        \n",
    "        print(f\"x_new: \\n {sum(x_new)}\")\n",
    "        residual = y - A@x_new\n",
    "        x_iters.append(x_new)\n",
    "        \n",
    "        err = np.linalg.norm(A@x_new-y,2) #l2-error\n",
    "        err_iters[i] = err\n",
    "        print(f\"err iteration {i}: {err}\")\n",
    "    \n",
    "    return x_new, x_iters, err_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 100\n",
    "x_hat, x_iters, err_iters = omp(A,y,n_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = np.linalg.norm(x_hat-x_true,2) #l2-error\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\color{gray}{\\text{Algorithms}}$\n",
    "Alternative reconstruction\n",
    "methods include greedy-type methods such as orthogonal matching pursuit, as well\n",
    "6 1 An Invitation to Compressive Sensing\n",
    "as thresholding-based methods including iterative hard thresholding (Foucart p.6)\n",
    "\n",
    "####  $\\color{gray}{\\text{References}}$\n",
    "[1] Mathematical Introducation to Compressive Sensing, Simon Foucart, Holger Rauhut"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
